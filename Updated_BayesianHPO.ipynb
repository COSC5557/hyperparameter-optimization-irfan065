{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8aD8tzVzDOD6",
        "outputId": "afb36f49-576b-474a-e0cb-f9f4a1833f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/100.3 kB\u001b[0m \u001b[31m721.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.9.7 scikit-optimize-0.9.0\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 50603.554533845774), ('kernel', 'rbf')])\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 119)])\n",
            "Bayesian Optimization for Logistic Regression raised an exception: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 24), ('min_samples_leaf', 2), ('min_samples_split', 11)])\n",
            "SVM Accuracy: 0.63125\n",
            "Random Forest Accuracy: 0.71875\n",
            "Decision Tree Accuracy: 0.615625\n",
            "Best Model: Random Forest 0.71875\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import warnings\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "data = pd.read_csv('wineq.csv')\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Algorithm Selection\n",
        "models = [\n",
        "    ('SVM', SVC()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Decision Tree', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "# Hyperparameter Optimization using Bayesian Optimization\n",
        "param_grids = {\n",
        "    'SVM': {\n",
        "        'C': Real(1e-4, 1e+5, prior='log-uniform'),\n",
        "        'kernel': Categorical(['linear', 'rbf', 'poly', 'sigmoid'])\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': Integer(100, 200),\n",
        "        'max_depth': Integer(10, 30),\n",
        "        'min_samples_split': Integer(1, 10),\n",
        "        'min_samples_leaf': Integer(1, 10)\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
        "        'penalty': Categorical(['l1', 'l2']),\n",
        "        'solver': Categorical(['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': Integer(1, 100),\n",
        "        'min_samples_split': Integer(2, 15),\n",
        "        'min_samples_leaf': Integer(1, 7)\n",
        "    }\n",
        "}\n",
        "\n",
        "final_scores = {}\n",
        "best_params = {}\n",
        "\n",
        "for name, model in models:\n",
        "    try:\n",
        "        opt = BayesSearchCV(\n",
        "            model,\n",
        "            param_grids[name],\n",
        "            n_iter=10,\n",
        "            cv=3,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            random_state=0\n",
        "        )\n",
        "        opt.fit(X_train, y_train)\n",
        "        best_model = opt.best_estimator_\n",
        "        accuracy = best_model.score(X_test, y_test)\n",
        "        final_scores[name] = accuracy\n",
        "        best_params[name] = opt.best_params_\n",
        "        print(f\"{name} - Tuned Hyperparameters: {opt.best_params_}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Bayesian Optimization for {name} raised an exception: {e}\")\n",
        "\n",
        "best_model_name = max(final_scores, key=final_scores.get)\n",
        "for name, accuracy in final_scores.items():\n",
        "    print(f\"{name} Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Best Model:\", best_model_name, final_scores[best_model_name])\n"
      ]
    }
  ]
}