{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries"
      ],
      "metadata": {
        "id": "xJt79MtsDqhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy import stats\n",
        "import warnings\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.exceptions import FitFailedWarning\n"
      ],
      "metadata": {
        "id": "ugGZ9-tzD38v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Data"
      ],
      "metadata": {
        "id": "Tw75-XBnEQo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('wineq.csv')\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "fePeR-8UEckD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Selection"
      ],
      "metadata": {
        "id": "DsUhKw34FPEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = [\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('SVM', SVC()),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Decision Tree', DecisionTreeClassifier())\n",
        "]\n"
      ],
      "metadata": {
        "id": "0Y70ACnlFZll"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation and Model Evaluation"
      ],
      "metadata": {
        "id": "kqiJUMShFhXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "results = {}\n",
        "for name, model in models:\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    results[name] = scores"
      ],
      "metadata": {
        "id": "saRL5EJBFmzN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Selection"
      ],
      "metadata": {
        "id": "hbjtuoytHXP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_scores = {}\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    final_scores[name] = accuracy\n",
        "\n",
        "best_model = max(final_scores, key=final_scores.get)\n",
        "for name, accuracy in final_scores.items():\n",
        "    print(f\"{name} Accuracy: {accuracy}\")\n",
        "print(\"Best Model:\", best_model, final_scores[best_model])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMrvU8cHWaR",
        "outputId": "da8410da-8b43-491e-a072-40eae5290464"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.746875\n",
            "SVM Accuracy: 0.503125\n",
            "Logistic Regression Accuracy: 0.63125\n",
            "Decision Tree Accuracy: 0.690625\n",
            "Best Model: Random Forest 0.746875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "K0ACs4G1RuP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Hyperparameter Optimization for Logistic Regression\n",
        "logreg_param_grid = {\n",
        "    'C': [0.1, 0.5, 1, 5, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "\n",
        "logreg_optimized = GridSearchCV(LogisticRegression(max_iter=100), param_grid=logreg_param_grid, cv=5, scoring='accuracy')\n",
        "logreg_optimized.fit(X_train, y_train)\n",
        "best_logreg_model = logreg_optimized.best_estimator_\n",
        "logreg_accuracy = best_logreg_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best Logistic Regression Model:\")\n",
        "print(\"Hyperparameters:\", best_logreg_model.get_params())\n",
        "print(\"Accuracy:\", logreg_accuracy)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Hyperparameter Optimization for Decision Tree\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30, 50, 100],\n",
        "    'min_samples_split': [2, 5, 10, 15],\n",
        "    'min_samples_leaf': [1, 2, 4, 7]\n",
        "}\n",
        "\n",
        "dt_optimized = GridSearchCV(DecisionTreeClassifier(), param_grid=dt_param_grid, cv=5, scoring='accuracy')\n",
        "dt_optimized.fit(X_train, y_train)\n",
        "best_dt_model = dt_optimized.best_estimator_\n",
        "dt_accuracy = best_dt_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best Decision Tree Model:\")\n",
        "print(\"Hyperparameters:\", best_dt_model.get_params())\n",
        "print(\"Accuracy:\", dt_accuracy)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Hyperparameter Optimization for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_optimized = GridSearchCV(RandomForestClassifier(), param_grid=rf_param_grid, cv=5, scoring='accuracy')\n",
        "rf_optimized.fit(X_train, y_train)\n",
        "best_rf_model = rf_optimized.best_estimator_\n",
        "rf_accuracy = best_rf_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best Random Forest Model:\")\n",
        "print(\"Hyperparameters:\", best_rf_model.get_params())\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Hyperparameter Optimization for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 0.8, 2, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "svm_optimized = GridSearchCV(SVC(), param_grid=svm_param_grid, cv=5, scoring='accuracy')\n",
        "svm_optimized.fit(X_train, y_train)\n",
        "best_svm_model = svm_optimized.best_estimator_\n",
        "svm_accuracy = best_svm_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best SVM Model:\")\n",
        "print(\"Hyperparameters:\", best_svm_model.get_params())\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtdN6PPKIrsq",
        "outputId": "8adb6c90-defa-4f39-b2d0-1a5ebdcdebd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "75 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.53793199        nan 0.50273284 0.58327206\n",
            " 0.56529412 0.54887868 0.5089951  0.50664522        nan        nan\n",
            " 0.55512561        nan 0.50664522 0.58640012 0.57153493 0.56528186\n",
            " 0.5105576  0.50586397        nan        nan 0.57701287        nan\n",
            " 0.50742647 0.58561581 0.5676348  0.56997549 0.51133885 0.50586397\n",
            "        nan        nan 0.57309436        nan 0.50586397 0.58170956\n",
            " 0.57701593 0.57387561 0.51133885 0.50664522        nan        nan\n",
            " 0.57465993        nan 0.50586397 0.5817065  0.57154412 0.57700061\n",
            " 0.51133885 0.50586397]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Model:\n",
            "Hyperparameters: {'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
            "Accuracy: 0.625\n",
            "\n",
            "\n",
            "Best Decision Tree Model:\n",
            "Hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 30, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}\n",
            "Accuracy: 0.665625\n",
            "\n",
            "\n",
            "Best Random Forest Model:\n",
            "Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
            "Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Best SVM Model:\n",
            "Hyperparameters: {'C': 0.8, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
            "Accuracy: 0.6375\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Selection after GridSeach\n",
        "\n"
      ],
      "metadata": {
        "id": "kvAwMm8ZSBOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_scores = {\n",
        "    'Random Forest': rf_accuracy,\n",
        "    'SVM': svm_accuracy,\n",
        "    'Logistic Regression': logreg_accuracy,\n",
        "    'Decision Tree': dt_accuracy\n",
        "}\n",
        "\n",
        "best_model = max(final_scores, key=final_scores.get)\n",
        "\n",
        "for name, accuracy in final_scores.items():\n",
        "    print(f\"{name} Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Best Model:\", best_model, final_scores[best_model])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFGfAnscNoRs",
        "outputId": "37202dab-7175-440b-b1e9-583406936077"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.703125\n",
            "SVM Accuracy: 0.6375\n",
            "Logistic Regression Accuracy: 0.625\n",
            "Decision Tree Accuracy: 0.665625\n",
            "Best Model: Random Forest 0.703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOMjG0sIarlB",
        "outputId": "6b9ed058-b647-4471-f17d-fa19bc29b7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.9.7 scikit-optimize-0.9.0\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.635\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 1.9360537965638225), ('kernel', 'linear')]), Accuracy: 0.59\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.66\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.61\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.6\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.54\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.595\n",
            "SVM - Tuned Hyperparameters: OrderedDict([('C', 46915.05705722399), ('kernel', 'rbf')]), Accuracy: 0.5829145728643216\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.74\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.71\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.715\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.7\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.7\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.595\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.7\n",
            "Random Forest - Tuned Hyperparameters: OrderedDict([('max_depth', 21), ('min_samples_leaf', 2), ('min_samples_split', 4), ('n_estimators', 46)]), Accuracy: 0.7185929648241206\n",
            "Bayesian Optimization for Logistic Regression raised an exception: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 53), ('min_samples_leaf', 1), ('min_samples_split', 6)]), Accuracy: 0.695\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 38), ('min_samples_leaf', 4), ('min_samples_split', 5)]), Accuracy: 0.605\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 53), ('min_samples_leaf', 1), ('min_samples_split', 6)]), Accuracy: 0.575\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 53), ('min_samples_leaf', 1), ('min_samples_split', 6)]), Accuracy: 0.62\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 53), ('min_samples_leaf', 1), ('min_samples_split', 6)]), Accuracy: 0.56\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 10), ('min_samples_leaf', 6), ('min_samples_split', 12)]), Accuracy: 0.615\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 10), ('min_samples_leaf', 6), ('min_samples_split', 12)]), Accuracy: 0.585\n",
            "Decision Tree - Tuned Hyperparameters: OrderedDict([('max_depth', 53), ('min_samples_leaf', 1), ('min_samples_split', 6)]), Accuracy: 0.6130653266331658\n",
            "SVM Outer CV Mean Accuracy: 0.6016143216080403\n",
            "Random Forest Outer CV Mean Accuracy: 0.6973241206030151\n",
            "Logistic Regression Outer CV Mean Accuracy: nan\n",
            "Decision Tree Outer CV Mean Accuracy: 0.6085081658291457\n",
            "Best Outer Model: Random Forest 0.6973241206030151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-8633489edd76>:84: RuntimeWarning: Mean of empty slice\n",
            "  mean_accuracy = np.nanmean(scores)  # Handling NaN values\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "data = pd.read_csv('wineq.csv')\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Outer loop for nested cross-validation\n",
        "outer_cv = 8\n",
        "inner_cv = 8\n",
        "outer_scores = {}\n",
        "\n",
        "# Algorithm Selection\n",
        "models = [\n",
        "    ('SVM', SVC()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Decision Tree', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "# Hyperparameter Optimization using Bayesian Optimization\n",
        "param_grids = {\n",
        "    'SVM': {\n",
        "         'C': Real(1e-5, 1e+5, prior='log-uniform'),\n",
        "        'kernel': Categorical(['linear', 'rbf', 'poly', 'sigmoid'])\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(10, 30),\n",
        "        'min_samples_split': Integer(1, 10),\n",
        "        'min_samples_leaf': Integer(1, 10)\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
        "        'penalty': Categorical(['l1', 'l2']),\n",
        "        'solver': Categorical(['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': Integer(1, 100),\n",
        "        'min_samples_split': Integer(2, 15),\n",
        "        'min_samples_leaf': Integer(1, 7)\n",
        "    }\n",
        "}\n",
        "\n",
        "for name, model in models:\n",
        "    try:\n",
        "        outer_scores[name] = []\n",
        "        for outer_train_index, outer_test_index in StratifiedKFold(n_splits=outer_cv, shuffle=True, random_state=0).split(X, y):\n",
        "            X_outer_train, X_outer_test = X[outer_train_index], X[outer_test_index]\n",
        "            y_outer_train, y_outer_test = y[outer_train_index], y[outer_test_index]\n",
        "\n",
        "            # Inner loop for hyperparameter tuning\n",
        "            opt = BayesSearchCV(\n",
        "                model,\n",
        "                param_grids[name],\n",
        "                n_iter=10,\n",
        "                cv=StratifiedKFold(n_splits=inner_cv, shuffle=True, random_state=0),\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1,\n",
        "                random_state=0\n",
        "            )\n",
        "            opt.fit(X_outer_train, y_outer_train)\n",
        "            best_model = opt.best_estimator_\n",
        "\n",
        "            # Evaluate on the outer test set\n",
        "            accuracy = best_model.score(X_outer_test, y_outer_test)\n",
        "            outer_scores[name].append(accuracy)\n",
        "\n",
        "            print(f\"{name} - Tuned Hyperparameters: {opt.best_params_}, Accuracy: {accuracy}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Bayesian Optimization for {name} raised an exception: {e}\")\n",
        "\n",
        "# Display results\n",
        "for name, scores in outer_scores.items():\n",
        "    mean_accuracy = np.nanmean(scores)  # Handling NaN values\n",
        "    print(f\"{name} Outer CV Mean Accuracy: {mean_accuracy}\")\n",
        "\n",
        "best_outer_model_name = max(outer_scores, key=outer_scores.get)\n",
        "print(\"Best Outer Model:\", best_outer_model_name, np.mean(outer_scores[best_outer_model_name]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import numpy as np\n",
        "\n",
        "model_results = [outer_scores[model_name] for model_name in outer_scores]\n",
        "\n",
        "flat_results = [score for scores in model_results for score in scores]\n",
        "\n",
        "\n",
        "labels = []\n",
        "for model_name, scores in outer_scores.items():\n",
        "    labels.extend([model_name] * len(scores))\n",
        "\n",
        "\n",
        "tukey_results = pairwise_tukeyhsd(np.array(flat_results), labels, alpha=0.05)\n",
        "from tabulate import tabulate\n",
        "tukey_df = pd.DataFrame(data=tukey_results._results_table.data[1:], columns=tukey_results._results_table.data[0])\n",
        "print(tabulate(tukey_df, headers='keys', tablefmt='pretty', showindex=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpLkxy-4J7Ya",
        "outputId": "220ce9bc-3ade-4f78-e177-d794ebf80a1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+----------+--------+---------+---------+--------+\n",
            "|    group1     |    group2     | meandiff | p-adj  |  lower  |  upper  | reject |\n",
            "+---------------+---------------+----------+--------+---------+---------+--------+\n",
            "| Decision Tree | Random Forest |  0.0888  | 0.0007 | 0.0382  | 0.1394  |  True  |\n",
            "| Decision Tree |      SVM      | -0.0069  | 0.9373 | -0.0575 | 0.0437  | False  |\n",
            "| Random Forest |      SVM      | -0.0957  | 0.0003 | -0.1463 | -0.0451 |  True  |\n",
            "+---------------+---------------+----------+--------+---------+---------+--------+\n"
          ]
        }
      ]
    }
  ]
}